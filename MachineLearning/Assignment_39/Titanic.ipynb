{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 4,
    "colab": {
      "name": "Titanic_Vscode.ipynb",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEYu_S2LdPxf"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "import tensorflow as tf"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5UFNWOBdafZ",
        "outputId": "e0001f4d-0fc7-4939-9fa7-ddb68396b606"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "Dt-H4BgOdPxi",
        "outputId": "04b82c21-90d0-4df1-ea31-57deebbc005f"
      },
      "source": [
        "train = pd.read_csv('drive/MyDrive/Colab Notebooks/Titanic Dataset/train.csv')\n",
        "train.head(2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Ticket</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Braund, Mr. Owen Harris</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>A/5 21171</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
              "      <td>female</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>PC 17599</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>C85</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PassengerId  Survived  Pclass  ...     Fare Cabin  Embarked\n",
              "0            1         0       3  ...   7.2500   NaN         S\n",
              "1            2         1       1  ...  71.2833   C85         C\n",
              "\n",
              "[2 rows x 12 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "hfPy4K_fdPxj",
        "outputId": "889432a3-d60c-422e-b231-010aa708eb70"
      },
      "source": [
        "\n",
        "#PreProcessing\n",
        "train= train.replace([\"female\" , \"male\"] , [0 , 1])\n",
        "train = train.replace([\"C\" , \"S\" , \"Q\"] , [0 , 1 , 2])\n",
        "train = train.fillna(0)\n",
        "train.head(2)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Ticket</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Braund, Mr. Owen Harris</td>\n",
              "      <td>1</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>A/5 21171</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
              "      <td>0</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>PC 17599</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>C85</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PassengerId  Survived  Pclass  ...     Fare  Cabin  Embarked\n",
              "0            1         0       3  ...   7.2500      0       1.0\n",
              "1            2         1       1  ...  71.2833    C85       0.0\n",
              "\n",
              "[2 rows x 12 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72tlBN_UdPxk",
        "outputId": "9ecfa39e-8f9e-4c17-92af-01feaddc4893"
      },
      "source": [
        "Y_train =  np.array(train[['Survived']])\n",
        "X_train = np.array(train[['Pclass', 'Sex', 'Fare', 'Age', 'SibSp', 'Parch', 'Embarked']])\n",
        "print(X_train.shape , Y_train.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(891, 7) (891, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1u8NeYMdPxl"
      },
      "source": [
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "                                    tf.keras.layers.Dense(7,activation='sigmoid'),\n",
        "                                    tf.keras.layers.Dense(8,activation='relu'),\n",
        "                                    tf.keras.layers.Dense(6,activation='relu'),\n",
        "                                    tf.keras.layers.Dense(4,activation='relu'),\n",
        "                                    tf.keras.layers.Dense(2,activation='softmax')\n",
        "])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQhOdRIKdPxn"
      },
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "              loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zwHMFbWdPxp",
        "outputId": "14f3cc3c-757a-4f79-a724-18b46565cfed"
      },
      "source": [
        "output=model.fit(X_train,Y_train,epochs=500)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "28/28 [==============================] - 1s 2ms/step - loss: 0.7811 - accuracy: 0.3838\n",
            "Epoch 2/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.7121 - accuracy: 0.3928\n",
            "Epoch 3/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.6796 - accuracy: 0.6263\n",
            "Epoch 4/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.6664 - accuracy: 0.6824\n",
            "Epoch 5/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.6596 - accuracy: 0.6779\n",
            "Epoch 6/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.6528 - accuracy: 0.6813\n",
            "Epoch 7/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.6450 - accuracy: 0.6756\n",
            "Epoch 8/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.6383 - accuracy: 0.6857\n",
            "Epoch 9/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.6304 - accuracy: 0.6846\n",
            "Epoch 10/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.6234 - accuracy: 0.6824\n",
            "Epoch 11/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.6159 - accuracy: 0.6790\n",
            "Epoch 12/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.6103 - accuracy: 0.6869\n",
            "Epoch 13/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.6017 - accuracy: 0.6958\n",
            "Epoch 14/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.5974 - accuracy: 0.6925\n",
            "Epoch 15/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.5909 - accuracy: 0.7003\n",
            "Epoch 16/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.5872 - accuracy: 0.7015\n",
            "Epoch 17/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.5835 - accuracy: 0.7015\n",
            "Epoch 18/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.5773 - accuracy: 0.7104\n",
            "Epoch 19/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.5722 - accuracy: 0.7127\n",
            "Epoch 20/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.5682 - accuracy: 0.7059\n",
            "Epoch 21/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.5650 - accuracy: 0.7059\n",
            "Epoch 22/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.5596 - accuracy: 0.7138\n",
            "Epoch 23/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.5564 - accuracy: 0.7194\n",
            "Epoch 24/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.5517 - accuracy: 0.7127\n",
            "Epoch 25/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.5433 - accuracy: 0.7183\n",
            "Epoch 26/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.5352 - accuracy: 0.7194\n",
            "Epoch 27/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.5292 - accuracy: 0.7250\n",
            "Epoch 28/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.5185 - accuracy: 0.7452\n",
            "Epoch 29/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.5161 - accuracy: 0.7654\n",
            "Epoch 30/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.5103 - accuracy: 0.7508\n",
            "Epoch 31/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.5038 - accuracy: 0.7565\n",
            "Epoch 32/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4996 - accuracy: 0.7699\n",
            "Epoch 33/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4914 - accuracy: 0.7744\n",
            "Epoch 34/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4885 - accuracy: 0.7710\n",
            "Epoch 35/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4852 - accuracy: 0.7733\n",
            "Epoch 36/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4822 - accuracy: 0.7722\n",
            "Epoch 37/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4804 - accuracy: 0.7722\n",
            "Epoch 38/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4753 - accuracy: 0.7879\n",
            "Epoch 39/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4772 - accuracy: 0.7811\n",
            "Epoch 40/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4696 - accuracy: 0.7868\n",
            "Epoch 41/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4718 - accuracy: 0.7901\n",
            "Epoch 42/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4695 - accuracy: 0.7901\n",
            "Epoch 43/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4722 - accuracy: 0.7811\n",
            "Epoch 44/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4630 - accuracy: 0.7890\n",
            "Epoch 45/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4654 - accuracy: 0.7856\n",
            "Epoch 46/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4613 - accuracy: 0.7912\n",
            "Epoch 47/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4590 - accuracy: 0.7890\n",
            "Epoch 48/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4574 - accuracy: 0.7924\n",
            "Epoch 49/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4572 - accuracy: 0.7879\n",
            "Epoch 50/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4565 - accuracy: 0.7845\n",
            "Epoch 51/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4535 - accuracy: 0.7957\n",
            "Epoch 52/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4532 - accuracy: 0.7957\n",
            "Epoch 53/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4525 - accuracy: 0.7969\n",
            "Epoch 54/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4502 - accuracy: 0.7912\n",
            "Epoch 55/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4495 - accuracy: 0.7935\n",
            "Epoch 56/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4511 - accuracy: 0.7845\n",
            "Epoch 57/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4536 - accuracy: 0.7924\n",
            "Epoch 58/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4685 - accuracy: 0.7868\n",
            "Epoch 59/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4547 - accuracy: 0.7957\n",
            "Epoch 60/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4522 - accuracy: 0.7912\n",
            "Epoch 61/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4462 - accuracy: 0.7946\n",
            "Epoch 62/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4457 - accuracy: 0.7924\n",
            "Epoch 63/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4447 - accuracy: 0.7890\n",
            "Epoch 64/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4483 - accuracy: 0.7924\n",
            "Epoch 65/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4503 - accuracy: 0.7946\n",
            "Epoch 66/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4475 - accuracy: 0.7946\n",
            "Epoch 67/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4478 - accuracy: 0.7957\n",
            "Epoch 68/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4450 - accuracy: 0.7912\n",
            "Epoch 69/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4431 - accuracy: 0.7935\n",
            "Epoch 70/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4443 - accuracy: 0.7924\n",
            "Epoch 71/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4436 - accuracy: 0.7834\n",
            "Epoch 72/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4433 - accuracy: 0.7924\n",
            "Epoch 73/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4502 - accuracy: 0.7879\n",
            "Epoch 74/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4496 - accuracy: 0.7890\n",
            "Epoch 75/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4425 - accuracy: 0.7879\n",
            "Epoch 76/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4390 - accuracy: 0.7935\n",
            "Epoch 77/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4389 - accuracy: 0.7991\n",
            "Epoch 78/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4375 - accuracy: 0.7924\n",
            "Epoch 79/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4374 - accuracy: 0.7935\n",
            "Epoch 80/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4348 - accuracy: 0.8036\n",
            "Epoch 81/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4395 - accuracy: 0.7912\n",
            "Epoch 82/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4435 - accuracy: 0.8002\n",
            "Epoch 83/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4453 - accuracy: 0.7901\n",
            "Epoch 84/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4345 - accuracy: 0.7980\n",
            "Epoch 85/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4340 - accuracy: 0.7991\n",
            "Epoch 86/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4329 - accuracy: 0.7935\n",
            "Epoch 87/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4341 - accuracy: 0.7980\n",
            "Epoch 88/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4409 - accuracy: 0.7991\n",
            "Epoch 89/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4445 - accuracy: 0.7980\n",
            "Epoch 90/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4381 - accuracy: 0.7946\n",
            "Epoch 91/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4332 - accuracy: 0.7957\n",
            "Epoch 92/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4313 - accuracy: 0.8036\n",
            "Epoch 93/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4305 - accuracy: 0.7912\n",
            "Epoch 94/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4309 - accuracy: 0.8025\n",
            "Epoch 95/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4313 - accuracy: 0.7901\n",
            "Epoch 96/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4311 - accuracy: 0.8025\n",
            "Epoch 97/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4272 - accuracy: 0.7980\n",
            "Epoch 98/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4308 - accuracy: 0.7980\n",
            "Epoch 99/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4287 - accuracy: 0.8025\n",
            "Epoch 100/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4293 - accuracy: 0.7879\n",
            "Epoch 101/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4273 - accuracy: 0.8013\n",
            "Epoch 102/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4297 - accuracy: 0.7980\n",
            "Epoch 103/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4278 - accuracy: 0.7969\n",
            "Epoch 104/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4317 - accuracy: 0.7890\n",
            "Epoch 105/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4293 - accuracy: 0.7980\n",
            "Epoch 106/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4349 - accuracy: 0.7868\n",
            "Epoch 107/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4256 - accuracy: 0.8013\n",
            "Epoch 108/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4250 - accuracy: 0.7980\n",
            "Epoch 109/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4345 - accuracy: 0.7980\n",
            "Epoch 110/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4255 - accuracy: 0.7980\n",
            "Epoch 111/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4260 - accuracy: 0.7980\n",
            "Epoch 112/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4280 - accuracy: 0.7969\n",
            "Epoch 113/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4262 - accuracy: 0.8025\n",
            "Epoch 114/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4221 - accuracy: 0.7901\n",
            "Epoch 115/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4247 - accuracy: 0.7946\n",
            "Epoch 116/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4244 - accuracy: 0.7924\n",
            "Epoch 117/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4222 - accuracy: 0.7912\n",
            "Epoch 118/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4226 - accuracy: 0.8058\n",
            "Epoch 119/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4215 - accuracy: 0.7991\n",
            "Epoch 120/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4226 - accuracy: 0.7980\n",
            "Epoch 121/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4199 - accuracy: 0.7969\n",
            "Epoch 122/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4192 - accuracy: 0.8025\n",
            "Epoch 123/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4261 - accuracy: 0.7935\n",
            "Epoch 124/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4173 - accuracy: 0.8047\n",
            "Epoch 125/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4202 - accuracy: 0.8047\n",
            "Epoch 126/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4213 - accuracy: 0.7969\n",
            "Epoch 127/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4200 - accuracy: 0.7991\n",
            "Epoch 128/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4197 - accuracy: 0.8002\n",
            "Epoch 129/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4184 - accuracy: 0.7969\n",
            "Epoch 130/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4165 - accuracy: 0.7991\n",
            "Epoch 131/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4182 - accuracy: 0.7969\n",
            "Epoch 132/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4191 - accuracy: 0.7924\n",
            "Epoch 133/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4217 - accuracy: 0.8036\n",
            "Epoch 134/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4173 - accuracy: 0.8025\n",
            "Epoch 135/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4178 - accuracy: 0.8126\n",
            "Epoch 136/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4173 - accuracy: 0.8103\n",
            "Epoch 137/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4150 - accuracy: 0.8092\n",
            "Epoch 138/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4175 - accuracy: 0.8013\n",
            "Epoch 139/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4148 - accuracy: 0.8114\n",
            "Epoch 140/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4153 - accuracy: 0.8025\n",
            "Epoch 141/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4150 - accuracy: 0.8002\n",
            "Epoch 142/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4140 - accuracy: 0.8013\n",
            "Epoch 143/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4150 - accuracy: 0.8114\n",
            "Epoch 144/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4219 - accuracy: 0.8002\n",
            "Epoch 145/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4165 - accuracy: 0.7946\n",
            "Epoch 146/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4163 - accuracy: 0.8126\n",
            "Epoch 147/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4125 - accuracy: 0.8047\n",
            "Epoch 148/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4170 - accuracy: 0.8058\n",
            "Epoch 149/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4140 - accuracy: 0.8126\n",
            "Epoch 150/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4135 - accuracy: 0.8002\n",
            "Epoch 151/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4187 - accuracy: 0.8025\n",
            "Epoch 152/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4151 - accuracy: 0.8047\n",
            "Epoch 153/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4168 - accuracy: 0.8025\n",
            "Epoch 154/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4146 - accuracy: 0.8081\n",
            "Epoch 155/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4139 - accuracy: 0.8002\n",
            "Epoch 156/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4119 - accuracy: 0.8036\n",
            "Epoch 157/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4114 - accuracy: 0.8103\n",
            "Epoch 158/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4155 - accuracy: 0.8092\n",
            "Epoch 159/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4103 - accuracy: 0.8103\n",
            "Epoch 160/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4134 - accuracy: 0.8025\n",
            "Epoch 161/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4134 - accuracy: 0.8013\n",
            "Epoch 162/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4098 - accuracy: 0.8058\n",
            "Epoch 163/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4088 - accuracy: 0.8092\n",
            "Epoch 164/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4090 - accuracy: 0.8081\n",
            "Epoch 165/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4132 - accuracy: 0.8070\n",
            "Epoch 166/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4079 - accuracy: 0.8070\n",
            "Epoch 167/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4072 - accuracy: 0.8092\n",
            "Epoch 168/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4110 - accuracy: 0.8058\n",
            "Epoch 169/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4080 - accuracy: 0.8070\n",
            "Epoch 170/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4082 - accuracy: 0.8103\n",
            "Epoch 171/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4100 - accuracy: 0.8148\n",
            "Epoch 172/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4095 - accuracy: 0.8092\n",
            "Epoch 173/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4099 - accuracy: 0.8204\n",
            "Epoch 174/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4087 - accuracy: 0.8081\n",
            "Epoch 175/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4067 - accuracy: 0.8126\n",
            "Epoch 176/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4070 - accuracy: 0.8114\n",
            "Epoch 177/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4053 - accuracy: 0.8047\n",
            "Epoch 178/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4058 - accuracy: 0.8193\n",
            "Epoch 179/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4061 - accuracy: 0.8148\n",
            "Epoch 180/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4121 - accuracy: 0.8036\n",
            "Epoch 181/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4045 - accuracy: 0.8204\n",
            "Epoch 182/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4101 - accuracy: 0.8137\n",
            "Epoch 183/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4036 - accuracy: 0.8159\n",
            "Epoch 184/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4099 - accuracy: 0.8092\n",
            "Epoch 185/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4066 - accuracy: 0.8126\n",
            "Epoch 186/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4125 - accuracy: 0.8070\n",
            "Epoch 187/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4119 - accuracy: 0.8092\n",
            "Epoch 188/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4127 - accuracy: 0.8159\n",
            "Epoch 189/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4158 - accuracy: 0.8047\n",
            "Epoch 190/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4081 - accuracy: 0.8092\n",
            "Epoch 191/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4090 - accuracy: 0.8182\n",
            "Epoch 192/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4069 - accuracy: 0.8070\n",
            "Epoch 193/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4089 - accuracy: 0.8215\n",
            "Epoch 194/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4191 - accuracy: 0.8092\n",
            "Epoch 195/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4079 - accuracy: 0.8148\n",
            "Epoch 196/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4029 - accuracy: 0.8193\n",
            "Epoch 197/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4031 - accuracy: 0.8215\n",
            "Epoch 198/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4200 - accuracy: 0.8047\n",
            "Epoch 199/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4120 - accuracy: 0.8193\n",
            "Epoch 200/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4037 - accuracy: 0.8103\n",
            "Epoch 201/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4057 - accuracy: 0.8204\n",
            "Epoch 202/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4035 - accuracy: 0.8137\n",
            "Epoch 203/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4036 - accuracy: 0.8159\n",
            "Epoch 204/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4058 - accuracy: 0.8193\n",
            "Epoch 205/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4016 - accuracy: 0.8182\n",
            "Epoch 206/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4035 - accuracy: 0.8171\n",
            "Epoch 207/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4106 - accuracy: 0.8148\n",
            "Epoch 208/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4055 - accuracy: 0.8193\n",
            "Epoch 209/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4073 - accuracy: 0.8182\n",
            "Epoch 210/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4025 - accuracy: 0.8238\n",
            "Epoch 211/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4071 - accuracy: 0.8114\n",
            "Epoch 212/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4010 - accuracy: 0.8204\n",
            "Epoch 213/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4033 - accuracy: 0.8204\n",
            "Epoch 214/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4068 - accuracy: 0.8171\n",
            "Epoch 215/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4017 - accuracy: 0.8238\n",
            "Epoch 216/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4020 - accuracy: 0.8148\n",
            "Epoch 217/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4068 - accuracy: 0.8182\n",
            "Epoch 218/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4047 - accuracy: 0.8182\n",
            "Epoch 219/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4006 - accuracy: 0.8215\n",
            "Epoch 220/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3987 - accuracy: 0.8305\n",
            "Epoch 221/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4005 - accuracy: 0.8227\n",
            "Epoch 222/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.3985 - accuracy: 0.8249\n",
            "Epoch 223/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4034 - accuracy: 0.8249\n",
            "Epoch 224/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4041 - accuracy: 0.8204\n",
            "Epoch 225/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4065 - accuracy: 0.8204\n",
            "Epoch 226/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4011 - accuracy: 0.8215\n",
            "Epoch 227/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3990 - accuracy: 0.8272\n",
            "Epoch 228/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4000 - accuracy: 0.8193\n",
            "Epoch 229/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3958 - accuracy: 0.8238\n",
            "Epoch 230/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3996 - accuracy: 0.8272\n",
            "Epoch 231/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4104 - accuracy: 0.8114\n",
            "Epoch 232/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4025 - accuracy: 0.8204\n",
            "Epoch 233/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3994 - accuracy: 0.8260\n",
            "Epoch 234/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4015 - accuracy: 0.8193\n",
            "Epoch 235/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3993 - accuracy: 0.8260\n",
            "Epoch 236/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.3993 - accuracy: 0.8238\n",
            "Epoch 237/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.3954 - accuracy: 0.8249\n",
            "Epoch 238/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4024 - accuracy: 0.8294\n",
            "Epoch 239/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3989 - accuracy: 0.8294\n",
            "Epoch 240/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4004 - accuracy: 0.8204\n",
            "Epoch 241/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4014 - accuracy: 0.8316\n",
            "Epoch 242/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4036 - accuracy: 0.8193\n",
            "Epoch 243/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4054 - accuracy: 0.8238\n",
            "Epoch 244/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3983 - accuracy: 0.8227\n",
            "Epoch 245/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4011 - accuracy: 0.8294\n",
            "Epoch 246/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3945 - accuracy: 0.8215\n",
            "Epoch 247/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4036 - accuracy: 0.8272\n",
            "Epoch 248/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3955 - accuracy: 0.8294\n",
            "Epoch 249/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3976 - accuracy: 0.8305\n",
            "Epoch 250/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3977 - accuracy: 0.8316\n",
            "Epoch 251/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4006 - accuracy: 0.8283\n",
            "Epoch 252/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4012 - accuracy: 0.8249\n",
            "Epoch 253/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3986 - accuracy: 0.8305\n",
            "Epoch 254/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3984 - accuracy: 0.8272\n",
            "Epoch 255/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3965 - accuracy: 0.8316\n",
            "Epoch 256/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3998 - accuracy: 0.8272\n",
            "Epoch 257/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3963 - accuracy: 0.8272\n",
            "Epoch 258/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4081 - accuracy: 0.8171\n",
            "Epoch 259/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4039 - accuracy: 0.8215\n",
            "Epoch 260/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3968 - accuracy: 0.8204\n",
            "Epoch 261/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3934 - accuracy: 0.8260\n",
            "Epoch 262/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3936 - accuracy: 0.8294\n",
            "Epoch 263/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3945 - accuracy: 0.8249\n",
            "Epoch 264/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3933 - accuracy: 0.8339\n",
            "Epoch 265/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3968 - accuracy: 0.8227\n",
            "Epoch 266/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3920 - accuracy: 0.8238\n",
            "Epoch 267/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3954 - accuracy: 0.8294\n",
            "Epoch 268/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3978 - accuracy: 0.8215\n",
            "Epoch 269/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3933 - accuracy: 0.8283\n",
            "Epoch 270/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3925 - accuracy: 0.8328\n",
            "Epoch 271/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4008 - accuracy: 0.8260\n",
            "Epoch 272/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3925 - accuracy: 0.8260\n",
            "Epoch 273/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.3976 - accuracy: 0.8249\n",
            "Epoch 274/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.3934 - accuracy: 0.8316\n",
            "Epoch 275/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3921 - accuracy: 0.8272\n",
            "Epoch 276/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.3937 - accuracy: 0.8204\n",
            "Epoch 277/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4021 - accuracy: 0.8227\n",
            "Epoch 278/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3962 - accuracy: 0.8305\n",
            "Epoch 279/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3956 - accuracy: 0.8238\n",
            "Epoch 280/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3965 - accuracy: 0.8283\n",
            "Epoch 281/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3923 - accuracy: 0.8305\n",
            "Epoch 282/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3946 - accuracy: 0.8227\n",
            "Epoch 283/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3983 - accuracy: 0.8316\n",
            "Epoch 284/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3920 - accuracy: 0.8283\n",
            "Epoch 285/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3952 - accuracy: 0.8294\n",
            "Epoch 286/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3946 - accuracy: 0.8305\n",
            "Epoch 287/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3940 - accuracy: 0.8215\n",
            "Epoch 288/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3931 - accuracy: 0.8316\n",
            "Epoch 289/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3909 - accuracy: 0.8249\n",
            "Epoch 290/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3927 - accuracy: 0.8238\n",
            "Epoch 291/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3923 - accuracy: 0.8305\n",
            "Epoch 292/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3907 - accuracy: 0.8272\n",
            "Epoch 293/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3938 - accuracy: 0.8316\n",
            "Epoch 294/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.3902 - accuracy: 0.8328\n",
            "Epoch 295/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3920 - accuracy: 0.8305\n",
            "Epoch 296/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3967 - accuracy: 0.8328\n",
            "Epoch 297/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3969 - accuracy: 0.8159\n",
            "Epoch 298/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4104 - accuracy: 0.8249\n",
            "Epoch 299/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3904 - accuracy: 0.8350\n",
            "Epoch 300/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3908 - accuracy: 0.8249\n",
            "Epoch 301/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3964 - accuracy: 0.8260\n",
            "Epoch 302/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3980 - accuracy: 0.8238\n",
            "Epoch 303/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3916 - accuracy: 0.8316\n",
            "Epoch 304/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3935 - accuracy: 0.8361\n",
            "Epoch 305/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3911 - accuracy: 0.8350\n",
            "Epoch 306/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.3901 - accuracy: 0.8305\n",
            "Epoch 307/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3926 - accuracy: 0.8227\n",
            "Epoch 308/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3946 - accuracy: 0.8249\n",
            "Epoch 309/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3914 - accuracy: 0.8305\n",
            "Epoch 310/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3893 - accuracy: 0.8350\n",
            "Epoch 311/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3935 - accuracy: 0.8316\n",
            "Epoch 312/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3975 - accuracy: 0.8339\n",
            "Epoch 313/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3936 - accuracy: 0.8294\n",
            "Epoch 314/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4048 - accuracy: 0.8227\n",
            "Epoch 315/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4037 - accuracy: 0.8193\n",
            "Epoch 316/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3886 - accuracy: 0.8305\n",
            "Epoch 317/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3864 - accuracy: 0.8339\n",
            "Epoch 318/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3969 - accuracy: 0.8249\n",
            "Epoch 319/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3896 - accuracy: 0.8339\n",
            "Epoch 320/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3976 - accuracy: 0.8260\n",
            "Epoch 321/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3889 - accuracy: 0.8305\n",
            "Epoch 322/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3900 - accuracy: 0.8249\n",
            "Epoch 323/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3938 - accuracy: 0.8305\n",
            "Epoch 324/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3881 - accuracy: 0.8350\n",
            "Epoch 325/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3880 - accuracy: 0.8339\n",
            "Epoch 326/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3875 - accuracy: 0.8361\n",
            "Epoch 327/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3859 - accuracy: 0.8316\n",
            "Epoch 328/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3895 - accuracy: 0.8294\n",
            "Epoch 329/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3905 - accuracy: 0.8350\n",
            "Epoch 330/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3972 - accuracy: 0.8238\n",
            "Epoch 331/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3924 - accuracy: 0.8283\n",
            "Epoch 332/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3881 - accuracy: 0.8283\n",
            "Epoch 333/500\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.3952 - accuracy: 0.8328\n",
            "Epoch 334/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3881 - accuracy: 0.8373\n",
            "Epoch 335/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3938 - accuracy: 0.8328\n",
            "Epoch 336/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3885 - accuracy: 0.8316\n",
            "Epoch 337/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3891 - accuracy: 0.8361\n",
            "Epoch 338/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3885 - accuracy: 0.8305\n",
            "Epoch 339/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3884 - accuracy: 0.8316\n",
            "Epoch 340/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3940 - accuracy: 0.8305\n",
            "Epoch 341/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3879 - accuracy: 0.8328\n",
            "Epoch 342/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3906 - accuracy: 0.8350\n",
            "Epoch 343/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3883 - accuracy: 0.8294\n",
            "Epoch 344/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3880 - accuracy: 0.8339\n",
            "Epoch 345/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3943 - accuracy: 0.8294\n",
            "Epoch 346/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3981 - accuracy: 0.8316\n",
            "Epoch 347/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3890 - accuracy: 0.8294\n",
            "Epoch 348/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3979 - accuracy: 0.8249\n",
            "Epoch 349/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3880 - accuracy: 0.8350\n",
            "Epoch 350/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3868 - accuracy: 0.8272\n",
            "Epoch 351/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3881 - accuracy: 0.8339\n",
            "Epoch 352/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3890 - accuracy: 0.8350\n",
            "Epoch 353/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3900 - accuracy: 0.8305\n",
            "Epoch 354/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3872 - accuracy: 0.8283\n",
            "Epoch 355/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3894 - accuracy: 0.8373\n",
            "Epoch 356/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3866 - accuracy: 0.8305\n",
            "Epoch 357/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3870 - accuracy: 0.8350\n",
            "Epoch 358/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3873 - accuracy: 0.8328\n",
            "Epoch 359/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3867 - accuracy: 0.8328\n",
            "Epoch 360/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3913 - accuracy: 0.8260\n",
            "Epoch 361/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3961 - accuracy: 0.8305\n",
            "Epoch 362/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3862 - accuracy: 0.8384\n",
            "Epoch 363/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3880 - accuracy: 0.8316\n",
            "Epoch 364/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3894 - accuracy: 0.8316\n",
            "Epoch 365/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3902 - accuracy: 0.8294\n",
            "Epoch 366/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3888 - accuracy: 0.8384\n",
            "Epoch 367/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3894 - accuracy: 0.8339\n",
            "Epoch 368/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3848 - accuracy: 0.8384\n",
            "Epoch 369/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3908 - accuracy: 0.8350\n",
            "Epoch 370/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3873 - accuracy: 0.8339\n",
            "Epoch 371/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3887 - accuracy: 0.8373\n",
            "Epoch 372/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3858 - accuracy: 0.8294\n",
            "Epoch 373/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3889 - accuracy: 0.8305\n",
            "Epoch 374/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3858 - accuracy: 0.8350\n",
            "Epoch 375/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3861 - accuracy: 0.8373\n",
            "Epoch 376/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3993 - accuracy: 0.8328\n",
            "Epoch 377/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3939 - accuracy: 0.8339\n",
            "Epoch 378/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3914 - accuracy: 0.8272\n",
            "Epoch 379/500\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.3900 - accuracy: 0.8316\n",
            "Epoch 380/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3870 - accuracy: 0.8339\n",
            "Epoch 381/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3949 - accuracy: 0.8350\n",
            "Epoch 382/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3918 - accuracy: 0.8328\n",
            "Epoch 383/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3850 - accuracy: 0.8316\n",
            "Epoch 384/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3953 - accuracy: 0.8283\n",
            "Epoch 385/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3839 - accuracy: 0.8373\n",
            "Epoch 386/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3897 - accuracy: 0.8272\n",
            "Epoch 387/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3879 - accuracy: 0.8350\n",
            "Epoch 388/500\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.3880 - accuracy: 0.8429\n",
            "Epoch 389/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3876 - accuracy: 0.8361\n",
            "Epoch 390/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3888 - accuracy: 0.8294\n",
            "Epoch 391/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3868 - accuracy: 0.8418\n",
            "Epoch 392/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3881 - accuracy: 0.8316\n",
            "Epoch 393/500\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.3840 - accuracy: 0.8339\n",
            "Epoch 394/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3863 - accuracy: 0.8350\n",
            "Epoch 395/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3932 - accuracy: 0.8215\n",
            "Epoch 396/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3864 - accuracy: 0.8350\n",
            "Epoch 397/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3869 - accuracy: 0.8316\n",
            "Epoch 398/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3858 - accuracy: 0.8361\n",
            "Epoch 399/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3872 - accuracy: 0.8361\n",
            "Epoch 400/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3971 - accuracy: 0.8238\n",
            "Epoch 401/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3862 - accuracy: 0.8272\n",
            "Epoch 402/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3853 - accuracy: 0.8328\n",
            "Epoch 403/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3877 - accuracy: 0.8339\n",
            "Epoch 404/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3848 - accuracy: 0.8384\n",
            "Epoch 405/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3912 - accuracy: 0.8361\n",
            "Epoch 406/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4019 - accuracy: 0.8249\n",
            "Epoch 407/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4004 - accuracy: 0.8316\n",
            "Epoch 408/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3927 - accuracy: 0.8316\n",
            "Epoch 409/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3875 - accuracy: 0.8339\n",
            "Epoch 410/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3877 - accuracy: 0.8316\n",
            "Epoch 411/500\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.3863 - accuracy: 0.8305\n",
            "Epoch 412/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3841 - accuracy: 0.8350\n",
            "Epoch 413/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3896 - accuracy: 0.8328\n",
            "Epoch 414/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3861 - accuracy: 0.8316\n",
            "Epoch 415/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3905 - accuracy: 0.8305\n",
            "Epoch 416/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3825 - accuracy: 0.8339\n",
            "Epoch 417/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3862 - accuracy: 0.8373\n",
            "Epoch 418/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3846 - accuracy: 0.8350\n",
            "Epoch 419/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3853 - accuracy: 0.8361\n",
            "Epoch 420/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3876 - accuracy: 0.8373\n",
            "Epoch 421/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3904 - accuracy: 0.8384\n",
            "Epoch 422/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3865 - accuracy: 0.8373\n",
            "Epoch 423/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3869 - accuracy: 0.8294\n",
            "Epoch 424/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3843 - accuracy: 0.8339\n",
            "Epoch 425/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3837 - accuracy: 0.8328\n",
            "Epoch 426/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3841 - accuracy: 0.8350\n",
            "Epoch 427/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3828 - accuracy: 0.8328\n",
            "Epoch 428/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3803 - accuracy: 0.8339\n",
            "Epoch 429/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3842 - accuracy: 0.8395\n",
            "Epoch 430/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3820 - accuracy: 0.8305\n",
            "Epoch 431/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3899 - accuracy: 0.8305\n",
            "Epoch 432/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3805 - accuracy: 0.8384\n",
            "Epoch 433/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3959 - accuracy: 0.8215\n",
            "Epoch 434/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3833 - accuracy: 0.8339\n",
            "Epoch 435/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3810 - accuracy: 0.8406\n",
            "Epoch 436/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3864 - accuracy: 0.8339\n",
            "Epoch 437/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3868 - accuracy: 0.8305\n",
            "Epoch 438/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3816 - accuracy: 0.8395\n",
            "Epoch 439/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3835 - accuracy: 0.8316\n",
            "Epoch 440/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3832 - accuracy: 0.8361\n",
            "Epoch 441/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3838 - accuracy: 0.8418\n",
            "Epoch 442/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3865 - accuracy: 0.8339\n",
            "Epoch 443/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3966 - accuracy: 0.8294\n",
            "Epoch 444/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3868 - accuracy: 0.8373\n",
            "Epoch 445/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3848 - accuracy: 0.8316\n",
            "Epoch 446/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3835 - accuracy: 0.8328\n",
            "Epoch 447/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3817 - accuracy: 0.8316\n",
            "Epoch 448/500\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.3816 - accuracy: 0.8384\n",
            "Epoch 449/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3846 - accuracy: 0.8361\n",
            "Epoch 450/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3951 - accuracy: 0.8283\n",
            "Epoch 451/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3902 - accuracy: 0.8361\n",
            "Epoch 452/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3860 - accuracy: 0.8373\n",
            "Epoch 453/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3827 - accuracy: 0.8384\n",
            "Epoch 454/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3828 - accuracy: 0.8328\n",
            "Epoch 455/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3823 - accuracy: 0.8395\n",
            "Epoch 456/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3836 - accuracy: 0.8316\n",
            "Epoch 457/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3866 - accuracy: 0.8294\n",
            "Epoch 458/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3854 - accuracy: 0.8440\n",
            "Epoch 459/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3873 - accuracy: 0.8328\n",
            "Epoch 460/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3852 - accuracy: 0.8328\n",
            "Epoch 461/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3829 - accuracy: 0.8395\n",
            "Epoch 462/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3857 - accuracy: 0.8384\n",
            "Epoch 463/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3851 - accuracy: 0.8429\n",
            "Epoch 464/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3888 - accuracy: 0.8294\n",
            "Epoch 465/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3826 - accuracy: 0.8440\n",
            "Epoch 466/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3831 - accuracy: 0.8384\n",
            "Epoch 467/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3795 - accuracy: 0.8373\n",
            "Epoch 468/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3835 - accuracy: 0.8395\n",
            "Epoch 469/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3824 - accuracy: 0.8373\n",
            "Epoch 470/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3967 - accuracy: 0.8339\n",
            "Epoch 471/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3856 - accuracy: 0.8373\n",
            "Epoch 472/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3820 - accuracy: 0.8395\n",
            "Epoch 473/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3853 - accuracy: 0.8361\n",
            "Epoch 474/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3862 - accuracy: 0.8283\n",
            "Epoch 475/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3814 - accuracy: 0.8373\n",
            "Epoch 476/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3890 - accuracy: 0.8294\n",
            "Epoch 477/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3815 - accuracy: 0.8350\n",
            "Epoch 478/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3791 - accuracy: 0.8350\n",
            "Epoch 479/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3854 - accuracy: 0.8272\n",
            "Epoch 480/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3815 - accuracy: 0.8373\n",
            "Epoch 481/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3817 - accuracy: 0.8406\n",
            "Epoch 482/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3862 - accuracy: 0.8328\n",
            "Epoch 483/500\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.3885 - accuracy: 0.8361\n",
            "Epoch 484/500\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.3805 - accuracy: 0.8418\n",
            "Epoch 485/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3923 - accuracy: 0.8215\n",
            "Epoch 486/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3948 - accuracy: 0.8305\n",
            "Epoch 487/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3836 - accuracy: 0.8350\n",
            "Epoch 488/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3791 - accuracy: 0.8339\n",
            "Epoch 489/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3842 - accuracy: 0.8339\n",
            "Epoch 490/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3763 - accuracy: 0.8440\n",
            "Epoch 491/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3799 - accuracy: 0.8395\n",
            "Epoch 492/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3795 - accuracy: 0.8350\n",
            "Epoch 493/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3794 - accuracy: 0.8418\n",
            "Epoch 494/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3781 - accuracy: 0.8384\n",
            "Epoch 495/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3876 - accuracy: 0.8350\n",
            "Epoch 496/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3823 - accuracy: 0.8384\n",
            "Epoch 497/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3856 - accuracy: 0.8373\n",
            "Epoch 498/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3839 - accuracy: 0.8373\n",
            "Epoch 499/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3826 - accuracy: 0.8350\n",
            "Epoch 500/500\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3845 - accuracy: 0.8316\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "fY8SYNVNdPxr",
        "outputId": "08d3917e-07df-48a3-8600-63fb946d011a"
      },
      "source": [
        "\n",
        "plt.plot(output.history['loss'], c = 'red')\n",
        "plt.plot(output.history['accuracy'], c ='green')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Loss')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "vbA4L_aYdPxt",
        "outputId": "f37d41a3-3936-45f5-f518-1affc2d5e106"
      },
      "source": [
        "test = pd.read_csv('drive/MyDrive/Colab Notebooks/Titanic Dataset/test.csv')\n",
        "test = test.replace(['female','male'],[0, 1])\n",
        "test = test.replace(['S','C','Q'],[0, 1, 2])\n",
        "test=test.fillna(0)\n",
        "test.head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Ticket</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>892</td>\n",
              "      <td>3</td>\n",
              "      <td>Kelly, Mr. James</td>\n",
              "      <td>1</td>\n",
              "      <td>34.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>330911</td>\n",
              "      <td>7.8292</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>893</td>\n",
              "      <td>3</td>\n",
              "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
              "      <td>0</td>\n",
              "      <td>47.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>363272</td>\n",
              "      <td>7.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>894</td>\n",
              "      <td>2</td>\n",
              "      <td>Myles, Mr. Thomas Francis</td>\n",
              "      <td>1</td>\n",
              "      <td>62.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>240276</td>\n",
              "      <td>9.6875</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>895</td>\n",
              "      <td>3</td>\n",
              "      <td>Wirz, Mr. Albert</td>\n",
              "      <td>1</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>315154</td>\n",
              "      <td>8.6625</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>896</td>\n",
              "      <td>3</td>\n",
              "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
              "      <td>0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3101298</td>\n",
              "      <td>12.2875</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PassengerId  Pclass  ... Cabin  Embarked\n",
              "0          892       3  ...     0         2\n",
              "1          893       3  ...     0         0\n",
              "2          894       2  ...     0         2\n",
              "3          895       3  ...     0         0\n",
              "4          896       3  ...     0         0\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "U0z6gWQhdPxu",
        "outputId": "02f61878-2eb1-4e96-a5fe-032009f23618"
      },
      "source": [
        "survived = pd.read_csv('drive/MyDrive/Colab Notebooks/Titanic Dataset/gender_submission.csv')\n",
        "survived.head(2)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Survived</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>892</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>893</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PassengerId  Survived\n",
              "0          892         0\n",
              "1          893         1"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jmdrf17dPxv",
        "outputId": "b98d96ab-9570-485d-9427-bc8ce949bf13"
      },
      "source": [
        "\n",
        "X_test = np.array(test[['Pclass','Sex','Age','SibSp','Parch','Fare','Embarked']])\n",
        "Y_test = np.array(survived[['Survived']])\n",
        "print(X_train.shape , Y_train.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(891, 7) (891, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTPCVBN5dPxw"
      },
      "source": [
        "from keras import layers\n",
        "layer = layers.Normalization()\n",
        "layer.adapt(X_test)\n",
        "X_test = layer(X_test).numpy()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MF9QgswWdPxx",
        "outputId": "20d22f18-1d93-43b4-c931-a59c66efe42f"
      },
      "source": [
        "acc=model.evaluate(X_test,Y_test)\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14/14 [==============================] - 0s 1ms/step - loss: 0.1567 - accuracy: 0.9426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bgEuepKdPxy",
        "outputId": "0ebb6d98-205a-4f91-df51-f57b3d8abff6"
      },
      "source": [
        "Kelly=np.array([3,1,27,0,0,8.6,0])\n",
        "Kelly=Kelly.reshape(1,7)\n",
        "y_pred=model.predict(Kelly)\n",
        "prediction=np.argmax(y_pred)\n",
        "print('prediction  ----> ',prediction)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction  ---->  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZ1U1eoVdPxz"
      },
      "source": [
        "\n",
        "#___________KNN_____________#\n",
        "class kNearestNeighbors(): \n",
        "\n",
        "\n",
        "    def __init__(self, k):\n",
        "        self.k = k \n",
        "\n",
        "\n",
        "    def fit(self, X_train , y_train): \n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.number_classes = len(np.unique(y_train))\n",
        "\n",
        "\n",
        "    def nearNeighbors(self , X_test):\n",
        "        dist = np.sqrt(np.sum((X_test - self.X_train)**2 ,axis=1))\n",
        "        near_neighbors = np.argsort(dist)[:self.k]\n",
        "        return near_neighbors\n",
        "\n",
        "        \n",
        "    def predict (self, X_test):\n",
        "        near_neighbors = self.nearNeighbors(X_test)\n",
        "        y = np.argmax(np.bincount(self.y_train[near_neighbors]))\n",
        "        return y\n",
        "\n",
        "    def Evaluate (self, X_test, y_test):\n",
        "        Succes=0\n",
        "        Test_len = len(X_test)\n",
        "        for i in range (Test_len):\n",
        "            Prediction = self.predict(X_test[i])\n",
        "            if Prediction == y_test[i]:\n",
        "                Succes = Succes+1 \n",
        "        Accuracy = Succes / Test_len\n",
        "        return Accuracy"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ApLsg1TdPx1",
        "outputId": "664c29ed-33df-4b1b-f72c-946691ec9e65"
      },
      "source": [
        "Y_train = Y_train.reshape(891,)\n",
        "KNN = kNearestNeighbors(k=5)\n",
        "KNN.fit(X_train,Y_train)\n",
        "y = []\n",
        "for xts in X_test:\n",
        "    y.append(KNN.predict(xts))\n",
        "\n",
        "print(' Accuracy of ___________KNN_____________ is', KNN.Evaluate(X_test, Y_test))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy of ___________KNN_____________ is 0.6363636363636364\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zstbuow7dPx2"
      },
      "source": [
        "#___________Adaline_____________#\n",
        "\n",
        "from numpy.linalg import inv\n",
        "class AdalineClassifi(): \n",
        "\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def fit(self, x_train, y_train):\n",
        "        # W = (X.T * X)^-1 * (X.T * Y)\n",
        "        self.w=np.matmul(inv(np.matmul(x_train.T,x_train)),np.matmul(x_train.T,y_train))\n",
        "        return self.w\n",
        "        \n",
        "    def predict(self, x_test):\n",
        "        y_predict = np.matmul(x_test, self.w)\n",
        "        y=[]\n",
        "        for i in range(len(y_predict)):\n",
        "            if y_predict[i]<0.5:\n",
        "                y.append(0)\n",
        "            else:\n",
        "                y.append(1)  \n",
        "        y=np.array(y)\n",
        "        return y\n",
        "    \n",
        "    def Evaluate (self, X_test, Y_test):\n",
        "        pred = self.predict(X_test)\n",
        "        Succes=0\n",
        "        for i in range (len(X_test)):\n",
        "           if pred[i] == Y_test[i]:\n",
        "               Succes = Succes+1 \n",
        "           Accuracy = Succes / len(X_test)\n",
        "        return Accuracy"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RerhbStfdPx4",
        "outputId": "fd976c2d-d260-481e-a892-4adb23d325af"
      },
      "source": [
        "model = AdalineClassifi()\n",
        "model.fit(X_train,Y_train)\n",
        "print(model.predict(X_test))\n",
        "print(' Accuracy of ___________Adaline_____________ is', model.Evaluate(X_test, Y_test))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1\n",
            " 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0\n",
            " 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0\n",
            " 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0\n",
            " 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 1 1 1 1 1 0 0 0 0 0]\n",
            " Accuracy of ___________Adaline_____________ is 0.8157894736842105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qTiwPDVdPx4"
      },
      "source": [
        "#___________Perceptron_____________\n",
        "\n",
        "class perceptron:\n",
        "    def __init__ (self):\n",
        "        pass\n",
        "\n",
        "    def fit(self, X, Y  , epochs):\n",
        "\n",
        "        lr = 0.000001\n",
        "        self.w = np.random.rand(7)\n",
        "        self.b = np.random.rand(1)\n",
        "        self.Error = []\n",
        "        for i in range (epochs):\n",
        "            for i in range (X.shape[0]):\n",
        "                y_pred = np.matmul(X[i], self.w ) + self.b\n",
        "                e = Y[i] - y_pred\n",
        "                self.w += lr * X[i] * e\n",
        "                self.b += lr * e \n",
        "\n",
        "                Y_pred = np.matmul(X, self.w) + self.b\n",
        "                error = np.mean(np.abs(Y - Y_pred))\n",
        "                self.Error.append(error)\n",
        "        np.save('w', self.w)\n",
        "        np.save('b', self.b)\n",
        "        return self.Error\n",
        "\n",
        "    def predict (self, X):\n",
        "        w = np.load('w.npy')\n",
        "        b = np.load('b.npy')\n",
        "        Y_pred=np.matmul(X, w) + b\n",
        "        return  Y_pred\n",
        "\n",
        "    def evaluate(self, X, Y):\n",
        "        w = np.load('w.npy')\n",
        "        b = np.load('b.npy')\n",
        "        Y_pred = np.matmul(X, w) + b\n",
        "        evaluation = np.mean(np.abs(Y - Y_pred))\n",
        "        return evaluation"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLVWC0XmdPx6",
        "outputId": "444302ab-589f-405d-cb13-f7771ab30adc"
      },
      "source": [
        "model = perceptron()\n",
        "model.fit(X_train, Y_train ,2)\n",
        "model.predict(X_test)\n",
        "model.evaluate(X_test, Y_test)\n",
        "print('Accuracy of ___________Perceptron_____________ is:', model.evaluate(X_test, Y_test) )"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of ___________Perceptron_____________ is: 0.7558562516826366\n"
          ]
        }
      ]
    }
  ]
}